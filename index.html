<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="MARA: Mode Anchored Reward Augmentation for KL-Regularized RL - ICLR 2026">
  <meta name="description" content="We show KL-regularized RL provably causes mode collapse, and introduce MARA - a principled fix that achieves diversity without sacrificing quality.">
  <meta name="keywords" content="reinforcement learning, KL regularization, mode collapse, diversity, RLHF, LLM, distribution matching, MARA">
  <meta name="author" content="Anthony GX-Chen, Jatin Prakash, Jeff Guo, Rob Fergus, Rajesh Ranganath">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="NYU CILVR Lab">
  <meta property="og:title" content="MARA: KL-Regularized RL is Designed to Mode Collapse">
  <meta property="og:description" content="We show KL-regularized RL provably causes mode collapse, and introduce MARA - a principled fix that achieves diversity without sacrificing quality.">
  <meta property="og:url" content="https://im-ant.github.io/mara">
  <meta property="og:image" content="static/images/mara-target-illustration.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="MARA - Mode Anchored Reward Augmentation">
  <meta property="article:published_time" content="2026-01-01T00:00:00.000Z">
  <meta property="article:author" content="Anthony GX-Chen">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="reinforcement learning">
  <meta property="article:tag" content="mode collapse">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@AntChen_">
  <meta name="twitter:creator" content="@AntChen_">
  <meta name="twitter:title" content="MARA: KL-Regularized RL is Designed to Mode Collapse">
  <meta name="twitter:description" content="We show KL-regularized RL provably causes mode collapse, and introduce MARA - a principled fix that achieves diversity without sacrificing quality.">
  <meta name="twitter:image" content="static/images/mara-target-illustration.png">
  <meta name="twitter:image:alt" content="MARA - Mode Anchored Reward Augmentation">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="KL-Regularized Reinforcement Learning is Designed to Mode Collapse">
  <meta name="citation_author" content="GX-Chen, Anthony">
  <meta name="citation_author" content="Prakash, Jatin">
  <meta name="citation_author" content="Guo, Jeff">
  <meta name="citation_author" content="Fergus, Rob">
  <meta name="citation_author" content="Ranganath, Rajesh">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="International Conference on Learning Representations (ICLR)">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2510.20817.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>MARA: KL-Regularized RL is Designed to Mode Collapse | ICLR 2026</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>

  <!-- MathJax for equations -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "KL-Regularized Reinforcement Learning is Designed to Mode Collapse",
    "description": "We show KL-regularized RL provably causes mode collapse, and introduce MARA - a principled fix that achieves diversity without sacrificing quality.",
    "author": [
      {
        "@type": "Person",
        "name": "Anthony GX-Chen",
        "affiliation": {
          "@type": "Organization",
          "name": "New York University"
        }
      },
      {
        "@type": "Person",
        "name": "Jatin Prakash",
        "affiliation": {
          "@type": "Organization",
          "name": "New York University"
        }
      },
      {
        "@type": "Person",
        "name": "Jeff Guo",
        "affiliation": {
          "@type": "Organization",
          "name": "New York University"
        }
      },
      {
        "@type": "Person",
        "name": "Rob Fergus",
        "affiliation": {
          "@type": "Organization",
          "name": "New York University"
        }
      },
      {
        "@type": "Person",
        "name": "Rajesh Ranganath",
        "affiliation": {
          "@type": "Organization",
          "name": "New York University"
        }
      }
    ],
    "datePublished": "2026-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "ICLR 2026"
    },
    "url": "https://im-ant.github.io/mara",
    "image": "static/images/mara-target-illustration.png",
    "keywords": ["reinforcement learning", "KL regularization", "mode collapse", "diversity", "RLHF", "LLM"],
    "isAccessibleForFree": true
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">KL-Regularized Reinforcement Learning is Designed to Mode Collapse</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://im-ant.github.io/" target="_blank">Anthony GX-Chen</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://bicycleman15.github.io/" target="_blank">Jatin Prakash</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://guojeff.github.io/" target="_blank">Jeff Guo</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://cs.nyu.edu/~fergus/" target="_blank">Rob Fergus</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://cims.nyu.edu/~rajeshr/" target="_blank">Rajesh Ranganath</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>New York University &nbsp;&nbsp; <sup>2</sup>EPFL</span>
              <br>
              <span class="author-block" style="font-weight: 600; color: #4a4a4a;">ICLR 2026</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2510.20817" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://openreview.net/forum?id=flBRtdIihA" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-comments"></i>
                </span>
                <span>OpenReview</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure>
        <video poster="" id="teaser" autoplay controls muted loop height="100%" preload="metadata">
          <source src="static/videos/BimodalRevKLAnimation.mp4" type="video/mp4">
        </video>
        <figcaption class="has-text-centered" style="margin-top: 0.5rem; font-style: italic; color: #666;">
          Vanilla KL regularized RL target mode-collapsed distributions. MARA achieves uniform coverage over all reward modes.
        </figcaption>
      </figure>
      <h2 class="subtitle has-text-centered" style="margin-top: 1.5rem;">
        <strong>TL;DR:</strong> KL/entropy regularized RL often provably causes mode collapse. But there is a principled fix.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Classical intuitions cast minimizing reverse KL as "mode seeking" and forward KL as "mass covering". In KL-regularized reinforcement learning, however, the regularizer determines <em>both</em> the target distribution's shape <em>and</em> the divergence being implicitly minimized, making its role more nuanced than simply inducing generic mode-seeking or mass-covering behaviour. Specifically, the target distribution is defined jointly by the reward function, the reference model, the type of regularizer, and the regularization strength. We show that under common settings—such as low regularization strength and equal verifiable rewards—both forward and reverse KL regularization tend to specify target distributions whose mass concentrates on a single high-reward region. Thus, the objective itself <em>by construction</em> induces diversity collapse, regardless of the policy optimization algorithm used.
          </p>
          <p>
            Building on this perspective, we introduce a simple and scalable modification that rescales rewards to induce target distributions assigning substantial probability across <em>all</em> high-reward regions. This yields a principled objective that maintains high solution quality while achieving broad reward-mode coverage. Empirically, this approach improves post-training diversity and performance for Large Language Models and Chemical Language Models, and is effective with either forward or reverse KL regularization, while using either naively fails.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Section: RL as Distribution Matching -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">RL as Distribution Matching</h2>
        <div class="content has-text-justified">
          <p>
            Many works observe that RL leads to diversity collapse. We identify:
          </p>
          <ol>
            <li>A simple reason why this happens</li>
            <li>A principled research agenda to fundamentally fix it—via characterizing the <em>target distribution</em></li>
          </ol>
          <p>
            We know the optimal solution of KL-regularized RL is a Gibbs distribution:
          </p>
          <p class="has-text-centered" style="font-size: 1.1em;">
            $\pi^*(y) = G_\beta (y) \propto \pi_{\text{ref}}(y) \exp\left(\frac{R(y)}{\beta}\right)$
          </p>
          <p>
            The regularized policy gradient is exactly a reverse-KL gradient toward this target:
          </p>
          <p class="has-text-centered" style="font-size: 1.1em;">
            $\nabla \text{KL}(\pi \| G_\beta)$
          </p>
          <p>
            <strong>Takeaway:</strong> Regularized RL is just distribution matching to the target distribution $G_\beta$.
          </p>
        </div>
        <div class="columns is-centered" style="margin-top: 1.5rem;">
          <div class="column">
            <figure>
              <video poster="" autoplay controls muted loop height="100%" preload="metadata">
                <source src="static/videos/RevKL_OptimizationAnimation.mp4" type="video/mp4">
              </video>
              <figcaption class="has-text-centered" style="margin-top: 0.5rem; font-style: italic; color: #666;">
                Policy optimization under reverse-KL regularization minimizes a reverse-KL divergence to the target distribution $G_\beta$.
              </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Section: Shape of the Target -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">What is the Shape of the Target Distribution?</h2>
        <div class="content has-text-justified">
          <p>
            <strong>Key insight:</strong> We can <em>exactly</em> calculate the relative likelihood of any two samples in the optimal distribution (normalization constant cancels out).
          </p>
        </div>
        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/prob-ratio-proposition-41.png" alt="Probability ratio proposition" style="max-width: 100%; margin: 0 auto; display: block;">
        </figure>
        <div class="content has-text-justified">
          <p>
            This leads to a number of interesting observations about why mode collapse happens.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Section: The Problem -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Why Mode Collapse Happens</h2>
        
        <div class="content has-text-justified">
          <h4 class="title is-5">Problem 1: Exponential Probability Differences</h4>
          <p>
            Linear differences in rewards lead to <strong>exponential differences in probabilities</strong>. With low KL regularization (e.g., 0.001), we effectively have a <em>single</em> solution—no diversity.
          </p>
          <p>
            <em>Note:</em> Entropy regularization has this problem as well.
          </p>
        </div>
        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/same-piref-diff-reward.png" alt="Same reference, different rewards lead to exponential probability differences" style="max-width: 100%; margin: 0 auto; display: block;">
        </figure>

        <div class="content has-text-justified">
          <h4 class="title is-5">Problem 2: Reference Policy Bias Persists</h4>
          <p>
            If two samples have the <strong>same reward</strong> (common in RLVR), RL does not change their relative probabilities from $\pi_\text{ref}$ (reference policy).
          </p>
          <p>
            Lowering the regularization coefficient has <em>no effect</em> on increasing the probability of low-support solutions, relative to high-support ones.
          </p>
        </div>
        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/diff-piref-same-reward.png" alt="Different reference probabilities, same reward - bias persists" style="max-width: 100%; margin: 0 auto; display: block;">
        </figure>

        <div class="content has-text-justified">
          <p>
            <strong>Key insight:</strong> None of these issues are "quirks" of RL—it is a consequence of how we defined the objective. Collapse is the natural result of <em>correctly</em> matching the target.
          </p>
        </div>
        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/dist-match-kl-reg-rl.png" alt="RL as distribution matching to target distribution" style="max-width: 100%; margin: 0 auto; display: block;">
        </figure>
        <div class="content has-text-justified">
          <p>
            To prevent collapse, we can simply define a <strong>better target distribution</strong> to match to!
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Section: MARA Solution -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">MARA: Mode Anchored Reward Augmentation</h2>
        
        <div class="content has-text-justified">
          <p>
            The probability ratios give us a way to define better target distributions. With a bit of algebra one can see the following condition specifies when two samples will have the same probabilities under the target distribution for KL-regularized RL.
          </p>
        </div>

        <div class="columns is-centered" style="margin-bottom: 1.5rem;">
          <div class="column">
            <figure>
              <video poster="" autoplay controls muted loop height="100%" preload="metadata">
                <source src="static/videos/BiasedModesRevKL.mp4" type="video/mp4">
              </video>
              <figcaption class="has-text-centered" style="margin-top: 0.5rem; font-style: italic; color: #666;">
                The difference in rewards and $\pi_\text{ref}$ between two samples fully determines their relative probabilities under the target distribution $G_\beta$.
              </figcaption>
            </figure>
          </div>
        </div>
        
        <div class="content has-text-justified">
          <p>
            We ensure this condition is met for all "good" samples. In practice, this is achieved by simple augmentations to the reward and $\pi_\text{ref}$.
          </p>
          <p>
            <strong>Intuitively:</strong> We <em>increase</em> the reward for good samples with low $\pi_\text{ref}$, by referencing another good sample with high $\pi_\text{ref}$.
          </p>
          <p>
            We call this <strong>"Mode Anchored Reward Augmentation" (MARA)</strong>.
          </p>
        </div>
        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/mara-algorithm.png" alt="MARA algorithm" style="max-width: 100%; margin: 0 auto; display: block;">
        </figure>

        <div class="content has-text-justified">
          <p>
            The effect of MARA is a target policy which puts <strong>uniform mass</strong> <em>everywhere</em> there are good solutions, and sticks close to the reference when there are none.
          </p>
          <p>
            Optimizing this objective naturally yields a <strong>multimodal policy distribution</strong>.
          </p>
        </div>
        <figure class="image">
          <img src="static/images/mara-target-illustration.png" alt="MARA target distribution illustration" style="max-width: 100%; margin: 0 auto; display: block;">
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Section: Results -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Empirical Results</h2>
        
        <div class="content has-text-justified">
          <p>
            We test MARA across LLMs (verifiable + non-verifiable tasks) and a chemical generative model (molecular design).
          </p>
          <p>
            <strong>MARA is a drop-in change that improves diversity with no loss in quality</strong>, exactly as the theory predicts.
          </p>
        </div>
        <figure class="image" style="margin-bottom: 2rem;">
          <img src="static/images/empirical-results-mara.png" alt="Empirical results showing MARA improves diversity without sacrificing quality" style="max-width: 100%; margin: 0 auto; display: block;">
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Section: Forward KL Analysis -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Forward KL Analysis</h2>
        
        <div class="content has-text-justified">
          <p>
            We also analyzed the forward-KL regularized case. Interestingly:
          </p>
          <ul>
            <li>The gradient is <strong>not</strong> a forward-KL gradient</li>
            <li>There's a completely different target distribution (may not be mass covering!)</li>
          </ul>
          <p>
            <strong>Takeaway:</strong> We cannot naively use intuitions about forward-KL for forward-KL regularized RL.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column">
            <figure>
              <video poster="" autoplay controls muted loop height="100%" preload="metadata">
                <source src="static/videos/FwdKL_OptimizationAnimation.mp4" type="video/mp4">
              </video>
              <figcaption class="has-text-centered" style="margin-top: 0.5rem; font-style: italic; color: #666;">
                Policy optimization under forward-KL regularization does not naturally yield a multimodal policy distribution.
              </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Section: Conclusion -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Summary</h2>
        
        <div class="content has-text-justified">
          <p>
            By viewing RL through its target distribution, we can have more principled insights on everything from exploration to entropy collapse.
          </p>
          <p class="has-text-centered" style="font-size: 1.2em; font-weight: 600; margin-top: 1.5rem;">
            It's all just distribution matching.
          </p>
          <p style="margin-top: 1.5rem;">
            This is a useful perspective that opens up many opportunities for future works!
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="bibtex-header">
      <h2 class="title">BibTeX</h2>
      <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
        <i class="fas fa-copy"></i>
        <span class="copy-text">Copy</span>
      </button>
    </div>
      <pre id="bibtex-code"><code>@inproceedings{
  gxchen2026mara,
  title={KL-Regularized Reinforcement Learning is Designed to Mode Collapse},
  author={Anthony GX-Chen and Jatin Prakash and Jeff Guo and Rob Fergus and Rajesh Ranganath},
  booktitle={The Fourteenth International Conference on Learning Representations},
  year={2026},
  url={https://arxiv.org/abs/2510.20817}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</main>

</body>
</html>
